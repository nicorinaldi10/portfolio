<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>Projects - Pitch-Roll-Yaw Face Orientation</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Poppins:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="vendor/aos/aos.css" rel="stylesheet">
  <link href="vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="css/main.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Updated: Jun 29 2024 with Bootstrap v5.3.3
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body class="portfolio-details-page">

  <header id="header" class="header dark-background d-flex flex-column">
    <i class="header-toggle d-xl-none bi bi-list"></i>

    <div class="profile-img">
      <img src="img/my-profile-img.jpeg" alt="" class="img-fluid rounded-circle">
    </div>

    <a href="index.html" class="logo d-flex align-items-center justify-content-center">
      <!-- Uncomment the line below if you also wish to use an image logo -->
      <!-- <img src="/img/logo.png" alt=""> -->
      <h1 class="sitename">Nicolás Rinaldi</h1>
    </a>

    <div class="social-links text-center">      
      <a href="https://www.linkedin.com/in/nicolás-rinaldi-montes-150b49250" class="linkedin"><i class="bi bi-linkedin"></i></a>
      <a href="mailto:albertonicolasrinaldi@gmail.com" class="email-link"><i class="bi bi-envelope flex-shrink-0"></i></a>
      <a href="https://github.com/nicorinaldi10" class="github"><i class="bi bi-github"></i></a>
      <a href="https://www.researchgate.net/profile/Nicolas-Montes-11?ev=hdr_xprf" class="rg-icon">
        <svg class="rg-svg" width="20" height="20" viewBox="0 0 32 32" fill="currentColor" xmlns="http://www.w3.org/2000/svg">
          <path d="M 5 5 L 5 27 L 27 27 L 27 5 L 5 5 z M 7 7 L 25 7 L 25 25 L 7 25 L 7 7 z M 19.164062 10.001953 C 17.881062 10.001953 17.441406 10.919156 17.441406 11.535156 L 17.441406 13.169922 C 17.441406 13.999922 17.8935 14.792969 19.0625 14.792969 C 21.0245 14.790969 20.787109 13.306391 20.787109 12.275391 L 19.253906 12.275391 L 19.253906 12.816406 L 20.158203 12.816406 C 20.158203 13.621406 19.781453 14.162109 19.064453 14.162109 C 18.498453 14.162109 18.171875 13.671188 18.171875 12.992188 L 18.171875 11.634766 C 18.171875 11.005766 18.762062 10.642578 19.164062 10.642578 C 19.881062 10.642578 20.15625 11.271484 20.15625 11.271484 L 20.697266 10.90625 C 20.697266 10.90625 20.434062 10.001953 19.164062 10.001953 z M 13.583984 13.091797 C 12.678984 13.091797 11.296953 13.178906 10.001953 13.128906 L 10.001953 13.53125 C 10.781953 13.68225 11.107422 13.606281 11.107422 14.738281 L 11.107422 20.269531 C 11.107422 21.413531 10.780953 21.325563 10.001953 21.476562 L 10.001953 21.892578 C 10.378953 21.879578 11.031266 21.841797 11.697266 21.841797 C 12.326266 21.841797 13.144094 21.867578 13.496094 21.892578 L 13.496094 21.476562 C 12.490094 21.338562 12.1875 21.451531 12.1875 20.269531 L 12.1875 17.931641 C 12.5275 17.956641 12.817531 17.955078 13.269531 17.955078 C 14.124531 19.489078 14.94125 20.634781 15.40625 21.175781 C 16.24825 22.193781 17.594875 22.043578 17.921875 21.892578 L 17.921875 21.515625 C 17.418875 21.514625 16.914781 21.175437 16.550781 20.773438 C 15.934781 20.107437 15.104781 19.025641 14.425781 17.806641 C 15.557781 17.529641 16.400391 16.461297 16.400391 15.404297 C 16.400391 13.820297 15.179984 13.091797 13.583984 13.091797 z M 13.320312 13.730469 C 14.502313 13.730469 15.205078 14.346516 15.205078 15.478516 C 15.204078 16.586516 14.450359 17.326172 13.193359 17.326172 C 12.728359 17.326172 12.5145 17.314063 12.1875 17.289062 L 12.1875 13.767578 C 12.5145 13.729578 12.942312 13.730469 13.320312 13.730469 z"/>
        </svg>
      </a>
    </div>

    <nav id="navmenu" class="navmenu">
      <ul>
        <li><a href="index.html#hero" class="active"><i class="bi bi-house navicon"></i>Home</a></li>
        <li><a href="index.html#about"><i class="bi bi-person navicon"></i> About</a></li>
        <li><a href="index.html#resume"><i class="bi bi-file-earmark-text navicon"></i> Resume</a></li>
        <li><a href="index.html#portfolio"><i class="bi bi-images navicon"></i> Portfolio</a></li>
        <li><a href="index.html#contact"><i class="bi bi-envelope navicon"></i> Contact</a></li>
      </ul>
    </nav>
    

  </header>

  <main class="main">

    <!-- Page Title -->
    <div class="page-title dark-background">
      <div class="container d-lg-flex justify-content-between align-items-center">
        <h1 class="mb-2 mb-lg-0">Pitch-Roll-Yaw Face Orientation</h1>
        <nav class="breadcrumbs">
          <ol>
            <li><a href="index.html">Home</a></li>
            <li class="current">Face Orientation</li>
          </ol>
        </nav>
      </div>
    </div><!-- End Page Title -->

    <!-- Portfolio Details Section -->
    <section id="portfolio-details" class="portfolio-details section">

      <div class="container" data-aos="fade-up" data-aos-delay="100">
        <div class="portfolio-description" data-aos="fade-up" data-aos-delay="300">
        </div>
        <div class="row gy-12">          
          <div class="col-lg-12">
              <ul>
                <li><strong>Level</strong>: Big project (University Final Project)</li>
                <li><strong>GitHub Repository:</strong> --</li> 
                <li><strong>Grade</strong>: 9.5/10</li>
              </ul>      
         
            <div class="portfolio-description" data-aos="fade-up" data-aos-delay="300">
              <h11>What is Pitch-Roll-Yaw Face Orientation?</h11> 
              <br>
              <br>
              <p>The aim of the project is to develop an application
                capable of estimate Head Position based on 2D RGB images. The
                starting point of the study is a BIWI dataset of labeled images
                regarding their rotation matrices. From the rotation matrices,
                the three angles are extracted, and certain specific points called
                landmarks are found by using Mediapipe. This set of information
                is then transformed into features regarding the euclidean distance
                pairwise between all the landmark points. The extracted features
                are managed by using keras, and introduced in a neural net based
                in regression with ADAM optimizer and ReLU as activation
                function to obtain the pose estimation. Finally, estimation error
                and deviations are calculated and interpreted.
                </p>

                 <br>
    
              <strong>PRELUDE:</strong> <br>

              <br>

              <p>Head Pose Estimation studies the rotation angles from the
                human head. From now on it will be referred by its acronym
                HPE. The nose is considered as the center point (0,0,0), and
                rotations are by convention represented by the Motion Imagery
                Standards Board as pitch, yaw and roll. The directions of pitch,
                yaw and roll can be seen in the figure below. The followed steps for
                developing an HPE algorithm can be seen in that same figure.</p>
              

                  <div class="d-flex justify-content-center">
                    <img src="img/portfolio/fig1.PNG" alt="Imagen" width="300" height="130">
                  </div> <br>

              <p>The relevant data sets for this study are the ones that have
                2D RGB images, find a list of candidates for providing the
                ground data in the table below.</p> <br>

                <div class="d-flex justify-content-center">
                  <img src="img/portfolio/tab1.jpg" alt="Imagen" width="470" height="180">
                </div> <br>

                <p>It is important to use techniques to perform a previous
                processing of the data. This step is related to obtaining good
                landmarks by applying different methods. Then, the HPE
                method is going to be built and finally computation of error
                of yaw, pitch and roll angles.</p>

              
              <br>
    
              <strong>STATE OF ART:</strong> <br>
  
              <br>

              <p>There are two primary approaches. One solution to the
                issue is the traditional method, which extracts landmarks from
                the input images and then treats them with neural network
                techniques like regression. The extensive use of deep learning
                in this assignment would be the second.
                </p>

                

              <ui>
                <li><strong> 1) Classical Methods:</strong></li> 

                <p>The classical methods can be split into two different sub-approaches.</p>

                <p><strong>The first one</strong> involves <strong>2D appearance-based methods</strong>, by considering each training image as a high-dimensional vector and extracting its visual features given its statistical distribution. Then, when the model is ready, it should be able to distinguish between different head poses.</p>
                
                <p>In the last years, many methods based on a <strong>multiclass classification task</strong> have been used. By applying <strong>Gaussian derivatives</strong> to the image, the feature extraction is performed. After that, they are subjected to a <strong>pattern classification (SVM)</strong> that differentiates between different poses.</p>
                
                <p>Other methods consist of computing a <strong>nine-dimensional local descriptor</strong> for pixels of input pictures and then computing the <strong>magnitude and orientation of the gradient</strong>. Then, for each input, a <strong>global feature</strong> is calculated regarding the above-mentioned descriptors.</p>
                
                <p><strong>Holistic methods</strong> are significantly easy to implement and fit both for <strong>high- and low-resolution images</strong>, but they have some issues:</p>
                
                <ul>
                    <li>Having a correct <strong>head detection</strong> is a major conditioning factor for the algorithm to work properly.</li>
                    <li>The <strong>HPE task</strong> may become extremely complex when poses experience major changes in illumination and facial expressions, leading to the need for a <strong>massive amount of data</strong> for training the model.</li>
                    <li>These methods do not perform properly in <strong>real-world scenarios</strong> due to massive changes and the impossibility of dealing with <strong>occlusions</strong> in 2D pictures.</li>
                </ul>

                <br>
                
               
                
                <li><strong>2) Deep Learning Methods:</strong></li>

                <p><strong>Deep learning methods</strong> came up with solutions to the issues experienced by classical methods.</p>

                <p>One of these modern approaches was done by <strong>Ruiz</strong>, estimating the head pose by working with intensity differences through <strong>Convolutional Neural Networks (CNNs)</strong>. This approach requires estimating the landmarks before performing the technique.</p>

                <p><strong>H.W. Hsu</strong> performs pose estimation using a CNN based on a <strong>multi-regressive loss function</strong>, using a discrimination based on <strong>quaternions</strong>.</p>

                <p><strong>S. Lee and Saitoh</strong> use a well-known database called <strong>Pointing’04</strong> to train the CNN model and perform classification of different head poses across a broad range of <strong>pitch and yaw values</strong> from a single picture. The accuracy of these methods is high when working with data extracted under controlled conditions or synthetic samples. However, as with other methods, this accuracy <strong>drops</strong> when applied to real-world images.</p>

                <p><strong>A. Patacchiola and Cangelosi</strong> managed to solve the <strong>Head Pose Estimation</strong> challenge using real-world images by incorporating <strong>adaptive gradient techniques</strong>, <strong>dropout</strong>, and <strong>convolutional neural networks</strong>.</p>

                <p><strong>Hong</strong> developed a technique that, by leveraging <strong>regression neural models</strong>, reduced the challenges of <strong>multiview</strong> and <strong>multimodel</strong> to a single issue. This was achieved by enhancing classical convolutional layers with <strong>manifold regularization</strong>, improving feature representation learning while preserving the local attributes of neurons.</p>

                <p>Since the methods described above are still relatively <strong>new</strong>, the currently available hardware is <strong>not powerful enough</strong> to handle their computational cost efficiently. It will be interesting to monitor the <strong>performance improvements</strong> of these methods in the coming years.</p>

              </ui>

              <br>
    
              <strong>IMPLEMENTATION:</strong> <br>

              <br>


              <strong>A. Chosen Solution</strong>

              <p>The given solution in the present work follows the structure detailed on the <strong>figure below</strong>. Starting from choosing the correct way to perform the <strong>data acquisition and labeling</strong>, in this case by using the <strong>BIWI</strong> data set.</p>

              <p>After that, face pre-processing problems are tackled using the software <strong>Mediapipe</strong> for landmark extraction. This tool provides output-defined features based on <strong>Euclidean distances</strong> between each landmark and the other points in every picture.</p>

              <p>Finally, a <strong>neural network</strong> is constructed and trained using the <strong>Adam</strong> estimator. The model will then be evaluated based on an <strong>error assessment</strong>.</p> <br>

              <strong>B. Source Dataset</strong>
              <p>As mentioned above, the <strong>BIWI</strong> dataset created by Fanelli is chosen. This dataset contains over <strong>15,000 images</strong> of <strong>20 people</strong> (30% women and 70% men, with 4 individuals recorded twice).</p>

              <p>It provides a <strong>depth image</strong>, the corresponding <strong>RGB image</strong>, and its label for each frame. The head pose range covers approximately <strong>±75° yaw</strong> and <strong>±60° pitch</strong>. The ground truth data is assumed as the <strong>3D location</strong> of the head and its rotation.</p>

              <p>A <strong>Python script</strong> has been written to transform the <strong>source rotation matrix</strong> values into <strong>pitch, roll, and yaw</strong> angles for each image in the dataset.</p>

              <p>The obtained output is a file containing the <strong>labeled data</strong> for the three angles related to each image of the <strong>BIWI</strong> database.</p>

              <br>

              <script type="text/javascript" async
              src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
              </script>

              <div class="d-flex justify-content-center">
                  <p><strong>Rotation Matrix:</strong></p>
                  <br>
                  <p>\[
                  RotationMatrix =
                  \begin{bmatrix}
                  a_{11} & a_{12} & a_{13} \\
                  a_{21} & a_{22} & a_{23} \\
                  a_{31} & a_{32} & a_{33} 
                  \end{bmatrix}
                  \]
                  </p>
              </div>
              <br>
              
              <div class="d-flex justify-content-center"><p><strong>Roll:</strong></p> <br></div>
              
              <div class="d-flex justify-content-center">
                  <p>\[
                  Roll = \arctan \frac{a_{21}}{a_{11}}
                  \]
                  </p>
              </div>
              <br>

              <div class="d-flex justify-content-center"><p><strong>Pitch:</strong></p> <br></div>
              <div class="d-flex justify-content-center">
                  <p>\[
                  Pitch = \arctan \frac{a_{32}}{a_{33}}
                  \]
                  </p>
              </div>
              <br>

              <div class="d-flex justify-content-center"><p><strong>Yaw:</strong></p> <br></div>
              <div class="d-flex justify-content-center">
                  <p>\[
                  Yaw = \arctan \frac{a_{31}}{\sqrt{a_{32}^2 + a_{33}^2}}
                  \]
                  </p>
              </div>
              
              <br>
    
              <br>

              <br>
    
    
              <strong>A. Landmark Acquisition</strong>
              <br>
    
              <p>This stage makes the connections between the locations of the many <b>facial landmarks</b>—including the eyes, noses, eyebrows, lips, chins, and more—in a <b>2D image</b> and their positions in a <b>3D model</b>. The following stage requires that these markers to be aligned. The goal of <b>facial landmark identification</b> algorithms is to automatically locate the major <b>landmarks</b> on the face that best describe the precise location of a <b>facial component</b>. These algorithms can be divided into three primary groups:</p> <br>
    
              <ul>
    
                <li><b>Active Apaireance Model</b>: <b>AAM</b> was introduced by Taylor Cootes. It develop the global <b>facial shape model</b> and the holistic <b>facial appearance model</b> sequentially based on <b>Principal Component Analysis (PCA)</b> as can be seen in the figure below.</li> <br>
    
    
                <div class="d-flex justify-content-center">
                  <img src="img/portfolio/fig2.PNG" alt="Imagen" width="300" height="160">
                </div> <br>
    
                <li><b>Constrained Local Model (CLM)</b>: CLM methods predict the <b>landmark points</b> regarding the <b>global facial shape</b> patterns.</li> <br>
    
                  <div class="d-flex justify-content-center">
                    <img src="img/portfolio/fig3.png" alt="Imagen" width="300" height="110">
                  </div> <br>  
    
    
                <li><b>Regression-based methods</b>: They are divided into <b>direct regression methods</b>, <b>cascaded regression methods</b>, and <b>deep-learning based regression methods</b>.
                </li> <br>
    
                    <ul>
    
                      <li><b>Direct regression methods</b> approaches for direct regression without any initialization of <b>landmark sites</b>,
                        learn the direct mapping from image appearance to <b>face landmark locations</b>.
                      </li>
    
                      <li><b>Cascaded regression methods</b>: These techniques begin with a preliminary hunch about the positions of the <b>facial landmarks</b> (such as the mean face) and
                        gradually update the <b>landmark locations</b> throughout stages using various <b>regression functions</b> learned for various stages.</li> <br>
    
    
                        <div class="d-flex justify-content-center">
                          <img src="img/portfolio/fig4.png" alt="Imagen" width="290" height="90">
                        </div> <br>  
    
                    </ul>
                
                  In the current paper, we employed a <b>2D facial landmark tracker</b> as <b>Mediapipe</b>, which has a python library designed using a combination of <b>Constrained Local Model (CLM)</b>
                  and <b>regression cascade approaches</b>, to locate trustworthy <b>face points</b>. By providing the landmark’s index, the <b>468 Face landmarks</b> produced by the <b>Mediapipe model</b> can be
                  retrieved. is a <b>machine learning framework</b> that is open source. <br>

                  <br>
    
                
                  The mechanism of the <b>algorithm</b> is summarised below.<br>

                  <br>
    
                  <li>A <b>neural network</b> is used for estimating the locations of the <b>3D mesh vertices</b>, treating each vertex as a separate <b>landmark</b>. There are <b>468 points</b> in the <b>mesh topology</b>,
                  which are fixed in quads.</li> <br>
    
    
                  <div class="d-flex justify-content-center">
                    <img src="img/portfolio/fig5.png" alt="Imagen" width="300" height="300">
                  </div> <br> 
    
                  <li> Higher point densities have been assigned to regions that
                    are anticipated to have greater variety and significance in <b>human perception</b>.
                    </li>  <br>
    
                    <li> With the use of <b>Catmull-Clark subdivision</b> (Fig 6), it
                      is possible to construct a believable smooth surface representation. Using the <b>Catmull-Clark approach</b>, additional vertices are formed, which are referred to as ”<b>face points</b>” and ”<b>edge points</b>,” respectively. Face points are
                      the original vertices of each face’s barycenters.</li>  <br>
    
                    <li><b>Mediapipe</b> performs the <b>image processing</b> as follows:</li>  <br>
    
                    <ul>
    
                      <li>An extremely lightweight <b>face detector</b> (available
                      from <b>Google, Blazeface</b>) processes the entire frame from the <b>camera input</b> and produces <b>face bounding rectangles</b> as well as many <b>landmarks</b> (such as <b>eye centers</b>, tragion, and <b>nose tip</b>). In order to coordinate
                      the line linking the <b>eye centers</b> with the horizontal axis of the <b>facial rectangle</b>, landmarks are utilized to
                      rotate the rectangle.</li> <br>
    
                      <div class="d-flex justify-content-center">
                        <img src="img/portfolio/fig6.PNG" alt="Imagen" width="300" height="170">
                      </div> <br> 
    
                      <li>The input to the <b>mesh prediction neural network</b>
                        is created by cropping and resizing the rectangle
                        acquired in the previous step from the original image. The <b>3D landmark coordinates</b> generated by this
                        model are converted into a vector and then remapped
                        into the original image’s coordinate system. <br>
                        </li>
                        <br>
                    </ul>
                   
                    <li>An initial model is trained being supervised by two main
                      agents:</li>
    
                      <ul>
    
                        <li><b>Synthetic renderings</b> of a <b>3D Movie Maker</b> over the <b>facial rectangles</b> of <b>real-world pictures</b>.</li>
    
                        <li><b>2D landmarks</b> corresponding to a subset of few <b>mesh vertices</b> that take part into a set of <b>semantic contours</b>. <br>
                        </li>
                        <br>
    
                      </ul>
                    
                    <li>Finally, the <b>model prediction</b> is improved in an <b>iterative</b>
                      way.
                      </li> <br>
    
                      <p>This model provides a trustworthy and substantial number
                      of <b>landmarks</b> that can offer the best features for the given <b>HPE method</b>. The following stages were engaged in order to
                      develop the <b>algorithm</b> using <b>mediapipe</b> to prepare the <b>photos</b>
                      for obtaining the <b>61 needed landmarks</b> (In the next section
                      will be discussed why only <b>61</b>): </p>
    
                      <li>Activate <b>static image mode</b> to make the program know that the input is not a <b>video</b>.</li>
                      <li>Force <b>Mediapipe</b> to detect only <b>one face</b> per image.</li>
                      <li>Convert image colourspace from <b>BGR</b> to <b>RGB</b> (in order to match with <b>OpenCV specifications</b>).</li>
                      <li><b>Mesh</b> is computed in order to get <b>468 landmark points</b> per photo.</li>
                      <li>Iterate each point, taking into account that the <b>landmark location</b> is in <b>pixel coordinates</b> but in the percentage point of the image in x, y.</li>
                      <li><b>Filter points</b> in <b>Mediapipe</b> through the list of <b>61 key points</b> defined before. Then, the output will only refer to those points.</li>
         </ul>
    
    
         <br>
    
    
              <strong>B. Feature Engineering</strong>
              <br>
    
    
             <p>The act of choosing, modifying, and converting unprocessed data into <b>features</b> that can be applied in <b>supervised learning</b> is known as <b>feature engineering</b>. As the reader may already
              be aware, a ”<b>feature</b>” is any quantifiable input that may be
              incorporated into a <b>predictive model</b>. In this instance, the raw
              data are the <b>landmarks</b> in each image, which we should utilize
              to build a <b>feature set</b> and apply the simplest transformation
              possible.</p>
    
            <p>In the present work, <b>Euclidean distance</b> between all points
              one by one is going to be used. In the <b>Euclidean plane</b>, let
              point p have <b>cartesian coordinates</b> p1, p2 and let point q
              have coordinates q1, q2, then the <b>distance</b> between p and q
              is given by:</p> <br>
    
            <script type="text/javascript" async
                src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
            </script>
        
            <div class="d-flex justify-content-center">
            \[
            d(p, q) = \sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2}
            \]
            </div>  <br>
    
            <p>This leads into a massive set of <b>features</b> per image in the
            database, fruit of <b>combinatorics</b> of <b>468 pairwise points</b>:</p> <br>
            
            <script type="text/javascript" async
                    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
            </script>
    
            <div class="d-flex justify-content-center">
                \[
                C_{468}^{2} = \frac{468!}{2!(468 - 2)!} = 109278 \textit{ features}
                \]
            </div> <br>
    
            <p>The <b>Neural Net model</b> will face a capacity issue with with
              such a high of <b>features</b> as a set of alternative parameters, and
              it will be necessary to choose <b>61 significant landmarks</b> in
              order to apply <b>Euclidean distance</b>. With this modification, the
              new collection of <b>characteristics</b> for each image yields the
              best number for the training of the <b>neural network</b> </p> <br>
    
            
              <script type="text/javascript" async
                      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
              </script>
              
              <div class="d-flex justify-content-center">
                  \[
                  C_{61}^{2} = \frac{61!}{2!(61 - 2)!} = 1830 \textit{ features}
                  \]
              </div>
    
              <p>The chosen <b>landmarks</b> are the following: 1, 5, 6, 12, 15, 33,
                34, 37, 40, 41, 46, 52, 53, 55, 57, 65, 84, 91, 124, 133, 135,
                137, 144, 153, 158, 160, 170, 171, 179, 195, 197, 215, 219,
                237, 263, 264, 267, 270, 271, 276, 282, 283, 285, 287, 295,
                314, 321, 353, 362, 364, 366, 373, 380, 385, 387, 395, 396,
                403, 435, 439, 457.
                </p>
      
                <br>
    
    
            <strong>C. Neural Network</strong>
            <br>
    
            <p>In the current approach, <b>HPE estimation</b> entails building a <b>neural network</b> using <b>supervised learning</b>. Next, the qualities that were picked to create <b>NN</b> and assist in obtaining <b>HPE</b>
            will be discussed. The full structure of the network are shown
            in the figure below.</p> <br>
            
            <div class="d-flex justify-content-center">
                <img src="img/portfolio/fig7.PNG" alt="Imagen" width="665" height="245">
              </div> <br> 
    
    
              <p><strong>1) Neural Network Hyperparameters:</strong> Hyperparameters are
              variables that control the actual <b>training process</b>. For instance,
              deciding how many <b>hidden layers</b> to employ between the <b>input</b>
              and <b>output layers</b> and how many nodes each layer needs
              (which in this case is a <b>dense network</b>) is an element of
              configuring a <b>deep neural network</b>. These variables are not
              directly connected to the training set of data. They are <b>configuration variables</b>, and the hyperparameters that are utilized
              are explained below.</p> <br>
    
    
              <li><strong>Data for training, validating, and testing the model:</strong>  
                Deciding what portion of the data should be used for <b>training</b>, <b>validating</b>, and <b>testing</b> the network.  
                The chosen values are <strong>75%</strong> (training), <strong>15%</strong> (validation), and <strong>10%</strong> (testing).
            </li> <br>
            <li><strong>Batch size:</strong>  
                Selecting a compromise value that increases <b>learning speed</b> is required. It is decided to use <strong>all training photos</strong> as a <b>batch size</b> since the amount of <b>training data</b> is reasonable.
            </li> <br>
            <li><strong>Number of layers and neurons in each layer:</strong>  
                - There are <strong>1830 neurons</strong> in the first <b>input layer</b>.  
                - There are <strong>20 and 10 neurons</strong> in each of the two <b>hidden layers</b>.  
                - The final <b>output layer</b> has <strong>3 neurons</strong>, which is also the number of outputs of the model.
            </li> <br>
            <li><strong>Activation function:</strong>  
                Typically, an <b>activation function</b> is used to provide <strong>non-linearity</strong> to network modeling.  
                The stochastic gradient descent algorithm’s convergence can be significantly accelerated via <strong>ReLU</strong> (eq. 8).  
                It is important to correctly decide in which layers <strong>ReLU</strong> is going to be used; for the present model, it will be applied in the <strong>two hidden layers</strong>.
            </li> <br>
    
            <script type="text/javascript" async
                    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
            </script>
    
            <div class="d-flex justify-content-center">
                \[
                ReLU(x) =
                \begin{cases} 
                x, & x \geq 0 \\
                0, & x < 0
                \end{cases}
                \]
            </div>
    
            <li><b>Loss function</b>: Since it is the one utilized to determine
              the <b>gradient of error</b>, directs the algorithm to reduce
              <b>prediction error</b>. The chosen function is <b>Minimum Square Error</b> due to its properties that make more comfortable
              to compute gradients (see eq. 9).</li> <br>
            
              <script type="text/javascript" async
                      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
              </script>
              
              <div class="d-flex justify-content-center">
                  \[
                  MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - Y_i)^2
                  \]
              </div>
    
              <li><b>Learning rate (α)</b>: is a parameter of the <b>gradient descent</b>
                technique that determines how quickly the <b>algorithm</b>
                reaches (converges) the ideal weights. After testing, the
                final model chose (<b>α = 0.001</b>) as the empirical value.</li> <br>
    
                <li><b>Opimizer ADAM</b>: Default values for <b>Keras</b> parameters
                  are <b>Adagrad = False</b>, <b>α = 0.01</b>, <b>β1 = 0.9</b> , <b>β1 = 0.99</b>,
                  and <b>ϵ = 1e07</b>. In the finished version of our network,
                  <b>ADAM</b> has been utilized with <b>α = 0.001</b> and <b>Adagrad = True</b> to avoid sudden changes in <b>learning rate</b>. The
                  remaining settings are predetermined by default.</li> <br>
    
                <li><b>Epochs</b>: Care must be made to avoid <b>overtraining</b> the network; an iteration or epoch has ended when all <b>training data</b> has gone through the network once. After
                  reviewing the graphs, <b>5000 epochs</b> were determined to
                  be appropriate for the current study.</li> <br>
    
                <p><strong>2) Final Network Structure and Adjustment Process:</strong> The
                  current <b>neural network</b> is sequential, with <b>ReLU</b> serving as the
                  <b>activation function</b> and <b>dense connections</b> between <b>neurons</b>.
                  Regarding <b>gradient descent</b>, <b>MSE</b> serves as the <b>loss function</b>,
                  and <b>ADAM</b> serves as the <b>optimizer</b>. By experimenting with
                  changes of these variables, the number of <b>hidden layers</b>,
                  the number of <b>neurons</b> in the network, and the number of
                  <b>iterations</b> are adjusted. The comparison between the <b>MSE</b>
                  and the <b>ADAM optimizer</b> is used as a measure of how well
                  the first three variables fit together as an indicator of their
                  goodness of fit. In the preceding subsection, <b>hyperparameters</b>
                  are described. <br>

                  <br>
    
            <strong>D. Evaluation and Results</strong>
                          
            <p>In the figures below the results for the developed <b>neural network</b> can be appreciated. Is showed that from <b>100 to 200 epochs</b> on, <b>accuracy</b> and <b>error</b> don’t significantly improve or alter. Therefore, this ought to be the ideal number of epochs for fitting our <b>NN</b>. Even yet, by increasing the number of epochs to further improve the results because <b>overfitting</b> is obvious in plots.</p>
    
              <div class="d-flex justify-content-center gap-3">
                <img src="img/portfolio/fig8.png" alt="Imagen 1" height: 100px>
                <img src="img/portfolio/fig9.png" alt="Imagen 2" height: 100px>
            </div> <br>
    
            <p>Two big targets are made with three sub-plots, each of which
              corresponds to the position of the <b>outputs</b> over the <b>test data</b>,
              as a way to plot our results. The plot displays an <b>absolute error</b> in green, calculated as in equation below.</p> <br>
    
              <script type="text/javascript" async
                      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
              </script>
    
              <div class="d-flex justify-content-center">
                  \[
                  E_{ABS} = Y_{m,k} - y_{m,k}
                  \]
              </div>
    
              
              <p>The <b>mean error</b> for
              each <b>test set</b> is indicated in blue, which illustrates how much 
              it deviates from <b>zero error</b>, and in red above and below the
              <b>confidence interval</b> lines. Given the quantity of data provided
              by this test, mark the portions of the <b>confidence interval</b> where
              86.64 percent of the instances would fall in order to observe
              their dispersion </p> <br>
    
              <div class="d-flex justify-content-center">
                <img src="img/portfolio/fig10.png" alt="Imagen" width="900" height="390">
              </div> <br> 
    
              <p>Based on the <b>hyperparameters</b> described above, the given
                from <b>simulation stage</b> are shown in the table below.</p> <br>
    
    
              <div class="d-flex justify-content-center">
                <img src="img/portfolio/tab2.jpg" alt="Imagen" width="400" height="75">
              </div> <br> 
    
              
              <p>The achieved <b>accuracy</b> is 91.503% for <b>training</b>, 95.951 %
              for <b>validation</b> and 91.543 % for <b>testing</b>.</p>
    

            </div>
          </div>

        </div>

      </div>

    </section><!-- /Portfolio Details Section -->

  </main>

  <footer id="footer" class="footer position-relative light-background">

    <div class="container">

      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you've purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: [buy-url] -->
        <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>

  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="vendor/php-email-form/validate.js"></script>
  <script src="vendor/aos/aos.js"></script>
  <script src="vendor/typed.js/typed.umd.js"></script>
  <script src="vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="vendor/waypoints/noframework.waypoints.js"></script>
  <script src="vendor/glightbox/js/glightbox.min.js"></script>
  <script src="vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="js/main.js"></script>

</body>

</html>
